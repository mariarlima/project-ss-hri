{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_ml_analysis as ml # Check utils_ml_analysis.py for the implementation of the methods\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to download the required spaCy model for NLP tasks\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c930d",
   "metadata": {},
   "source": [
    "## 1. Linguistic feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this method to extract the following linguistic features from a text file\n",
    "\n",
    "def text_analysis_features(file_path, lang='en'):\n",
    "    results = []\n",
    "\n",
    "    if file_path.endswith(\".txt\"): \n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            content = file.read()\n",
    "        brunet_index, honors_statistic, cttr, pid, duplicate_proportion = ml.calculate_ling_nlp(content, lang=lang)\n",
    "        results.append({\n",
    "            'Brunet': brunet_index,\n",
    "            'Honore': honors_statistic,\n",
    "            'CTTR': cttr,\n",
    "            'PIDensity': pid,\n",
    "            'Duplic': duplicate_proportion\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "p = './data/ad-example.txt'\n",
    "df_ad_features = text_analysis_features(p)\n",
    "df_ad_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to load the precomputed linguistic features and inspect the dataframe\n",
    "liwc_train = pd.read_pickle('./data/train.pkl')\n",
    "liwc_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd576d33",
   "metadata": {},
   "source": [
    "## 2. Cognitive Impairment Detection (binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d526fc3",
   "metadata": {},
   "source": [
    "#### *10-fold crossvalidation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180303e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# TODO: Load liwc_train features and assign features (X) and labels (y)\n",
    "# TODO: Convert list of feature vectors into a 2D NumPy array for processing\n",
    "# TODO: Normalize features for model compatibility and improved performance using StandardScaler()\n",
    "# TODO: Apply fit_transform to the training data. Set X_scaled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check and complete the functions below in utils_ml_analysis.py\n",
    "models = ml.create_models()\n",
    "param_grids = ml.create_param_grids()\n",
    "\n",
    "results = []\n",
    "# TODO: Check and complete cross-validation and hyperparameter tuning using ml.crossval() under utils_ml_analysis.py\n",
    "for name, model in models.items():\n",
    "    result = ml.crossval(name, model, param_grids[name], X_scaled_train, y, feature_set = 'cv_hyperparam_liwc')\n",
    "    results.append(result)\n",
    "\n",
    "# Aggregate cross-validation results into a DataFrame df_eval_cv\n",
    "df_eval_cv = pd.DataFrame(results)\n",
    "df_eval_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run to save the cross-validation results\n",
    "\n",
    "# PATH_SAVE_DF = \"./data/\"\n",
    "# df_eval_cv.to_csv(PATH_SAVE_DF + \"results_cv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a0947",
   "metadata": {},
   "source": [
    "#### *Evaluation on test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_test = pd.read_pickle('./data/test.pkl')\n",
    "liwc_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53512ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Assign test set features and labels using ./data/test_liwc.pkl\n",
    "# TODO: Convert test features to 2D NumPy array\n",
    "# TODO: Apply training-set scaler object to normalize test features \n",
    "\n",
    "# Load best hyperparameters from previous cross-validation\n",
    "best_hyperparams = ml.load_best_params(feature_set = 'cv_hyperparam_liwc')\n",
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c5337",
   "metadata": {},
   "source": [
    "Uncomment to evaluate results with bootstrapping (ensure variable names are compatible with your previous code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_bootstrap, probs = ml.fit_and_evaluate_bootstrap_(best_hyperparams, X_scaled_train, y, X_scaled_test, y_test)\n",
    "# evaluation_bootstrap_df = pd.DataFrame(evaluation_bootstrap)\n",
    "# df = evaluation_bootstrap_df.round(3)\n",
    "# df[df.columns[1:]] = df[df.columns[1:]] * 100\n",
    "# results_dict = {}\n",
    "# for model_name in df.Model:\n",
    "#     results = ml.extract_results_classif_test(df[df.Model == model_name])\n",
    "#     results_dict[model_name] = results\n",
    "# results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f581a",
   "metadata": {},
   "source": [
    "Uncomment to save the probs dictionary as a pickle file for later processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2082d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'results_eval_probs'\n",
    "# with open(f'./data/{name}.pkl', 'wb') as f:\n",
    "#     pickle.dump(probs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2c296",
   "metadata": {},
   "source": [
    "#### *Compute performance metrics using GPT embeddings from the transcripts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74706f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/gpt_train.pkl\", 'rb') as f: # new\n",
    "    df_gpt_train = pickle.load(f)\n",
    "\n",
    "with open('./data/gpt_test.pkl', 'rb') as f: # new\n",
    "    df_gpt_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ml.create_models()\n",
    "param_grids = ml.create_param_grids()\n",
    "\n",
    "# TODO: Like before, perform 10-f cross-validation with hyperparameter tuning for each model\n",
    "# TODO: Convert results to DataFrame 'df_eval_cv' for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_SAVE_DF = \"./data/\"\n",
    "# df_eval_cv.to_csv(PATH_SAVE_DF + \"results_cv_gpt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b4a1c",
   "metadata": {},
   "source": [
    "#### *Compare and discuss results (NLP vs. GPT)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
