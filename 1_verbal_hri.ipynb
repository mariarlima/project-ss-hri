{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd00af3d",
   "metadata": {},
   "source": [
    "# Part I \n",
    "Objective: Create the **verbal human-robot interaction (HRI) pipeline** for a socially assistive robot, focusing on structured LLM prompt design and speech interfaces in the context of an **interactive cognitive task**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b7944",
   "metadata": {},
   "source": [
    "## Milestones for Today\n",
    "\n",
    "- Prompt engineering of structured cognitive task \n",
    "- Integrate TTS & STT\n",
    "- Extend implementation to improve verbal interaction: latency, interruptions, LLM autoevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389bf54",
   "metadata": {},
   "source": [
    "### Prompt Design and Modular LLM Pipeline\n",
    "\n",
    "- Design a structured cognitive prompt \n",
    "- Set up infrastructure to call the LLM and obtain a response using your `LLM` class (from `LLM.py`)\n",
    "- Complete `main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d171937",
   "metadata": {},
   "source": [
    "### Speech Processing (TTS/STT) Integration\n",
    "- Enable audio settings in `configuration.py`: set `enable_TTS` = True and `enable_STT` = True\n",
    "- Ensure your `.env` file contains valid API keys for OpenAI API (https://github.com/openai/openai-python) and Unreal Speech API (https://docs.unrealspeech.com/reference/python-sdk)\n",
    "- Edit `STT.py` to initialize local microphone and implement Whisper transcription (https://github.com/openai/whisper)\n",
    "- Edit `TSS.py` to connect to UnrealSpeech, convert text to speech, play audio, and return its duration\n",
    "- Define appropriate voice settings in `TTS_config` in `configuration.py`\n",
    "- Use threading in `main.py` to run TTS playback non-blocking, allowing the interaction loop to continue during audio output\n",
    "- Use queue module in `main.py` to receive the audio duration from TTS and enable synchronization with motion or timed interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69025f98",
   "metadata": {},
   "source": [
    "### Additional Implementation\n",
    "- Implement auto evaluation as a preventive measure to filter LLM-generated responses \n",
    "- Improve the latency from API calls, e.g., Realtime API\n",
    "- Handle interruptions (big challenge in verbal HRI among older adults and people affected by dementia)\n",
    "- Logging: Store conversation, contextual variables and audio logs for debugging, human feedback, or retraining\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
