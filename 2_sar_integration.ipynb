{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32f7d2c",
   "metadata": {},
   "source": [
    "# Part II\n",
    "Objective: Integrate the **verbal human-robot interaction (HRI) pipeline** with the socially assistive robot (SAR) Blossom, such that its generated behaviour/motion is synchronised with speech duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a0e4a",
   "metadata": {},
   "source": [
    "## Milestones for Today\n",
    "\n",
    "- Integrate multimodal SAR behaviour\n",
    "- Synchronise speech-motion duration \n",
    "- Extend implementation to improve verbal interaction (with target population and interactive cognitive task context in mind) and overall multimodal integration: latency, interruptions, LLM autoevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76919e77",
   "metadata": {},
   "source": [
    "### Sequence Generation\n",
    "- Inspect Blossom-Controller folder (public repo: https://github.com/interaction-lab/Blossom-Controller)\n",
    "- Run `create_sequences.py` and confirm there are new .json files under `\"./Blossom-Controller/blossom-public/blossompy/src/sequences/woody/cognitive/\"`\n",
    "- Inspect `sequence_metadata` from `configuration.py` with previously created motion sequences (using Blossom's 4 motors) and corresponding duration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf391a",
   "metadata": {},
   "source": [
    "### Sync speech & SAR behaviour\n",
    "- Implement code in `blossom_wrapper.py` to select a robot motion sequence that best matches the duration of TTS audio output\n",
    "- Retrieve audio duration after TTS playback using signal_queue in `main.py` \n",
    "- Use threading in `main.py` to activate Blossom motion concurrently with TTS output. This enables synchronised verbal and nonverbal robot behavior without blocking the main interaction loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601fc2df",
   "metadata": {},
   "source": [
    "### Test Blossom\n",
    "\n",
    "- Connect the U2D2 converter from your laptop's USB port to one of Blossom’s four motor ports using the appropriate 3-pin JST cable\n",
    "- Ensure power is supplied to the robot and that the motor cables are securely connected to the U2D2\n",
    "- Plug in the Blossom robot via USB and wait a few seconds for the system to register the device\n",
    "- Open terminal and run: `ls /dev/tty.usb*` to confirm that the U2D2 interface is detected (e.g., /dev/tty.usbserial-FTAAMOPF)\n",
    "- Note: If the device doesn’t appear, check USB power, try another port, or unplug/replug the U2D2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be6a2c8",
   "metadata": {},
   "source": [
    "### Additional implementation tasks & thoughts \n",
    "- Improve response latency to enable natural conversational flow, e.g., Realtime API\n",
    "- Handle speech interruptions and turn-taking, a key challenge in verbal HRI with older adults and people living with dementia\n",
    "- Logging: Store conversation, contextual variables and audio logs for debugging, human feedback, or retraining\n",
    "- Implement LLM auto-evaluation to filter or rank inappropriate or incoherent responses in real time, in the context of an interactive cognitive tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
